\documentclass{beamer}

\mode<presentation>
{
  \usetheme{Darmstadt}
  % or ...

  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}
\setbeamertemplate{navigation symbols}{} 

\usepackage[utf8x]{inputenc}
\usepackage{amsmath} 
\usepackage{multicol}
\usepackage{multimedia,pgf,xmpmulti,listings}
\usepackage{amsmath,amssymb,latexsym}

\definecolor{orangeish}{rgb}{1,0.45,0}
\definecolor{ForestGreen}{rgb}{.25,0.45,0.25}

\usepackage{listings}

\lstset{
  language=C++,
  keywordstyle=\color{blue},
  identifierstyle=\color{ForestGreen},
  commentstyle=\color{red},
  stringstyle=\color{orangeish},
  frame=leftline,
  showstringspaces=false}

\title{Practical SIMD acceleration with Boost.SIMD}

\author{Mathias Gaunard \and Joël Falcou \and Jean-Thierry Lapresté}
\institute[MetaScale]{MetaScale Inc.}
\date[BOOST 2011]{Boostcon 2011}
\subject{Computer Science}

\pgfdeclareimage[height=0.5cm]{university-logo}{logo}
\logo{\pgfuseimage{university-logo}}

\begin{document}

\include{slideslst}

\begin{frame}
\titlepage
\end{frame}

\section{Introduction}

\subsection{Context}
\begin{frame}
	\frametitle{Context}

	\begin{itemize}
		\item Last year, we presented $NT^2$, a \textsc{Matlab}-like Proto-based library
		      for high-performance numerical computation
		\item Boost.SIMD is the extraction of the SIMD subcomponent of the
		      library
		\item GSoC project this summer to help make it ready for review
	\end{itemize}

\end{frame}

\subsection{SIMD}
\begin{frame}
	\frametitle{What's SIMD?}

	\begin{multicols}{2}

	\begin{itemize}
		\item Single Instruction, Multiple Data
		\item Operations you can perform on NxT elements
		      packed within a single register
		\item Up to N times faster than doing it element per element
		      with the ALU or the FPU
	\end{itemize}
	
	%Dessin
	
	\end{multicols}

\end{frame}

\subsection{SIMD abstraction}
\begin{frame}
	\frametitle{Why is SIMD abstraction needed?}
	
	\begin{multicols}{2}
	
		x86 family
		\begin{itemize}
			\item MMX 64-bit float, double
			\item SSE 128-bit float
			\item SSE2 128-bit int8, int16, int32, int64, double
			\item SSE3
			\item SSSE3
			\item SSE4a (AMD only)
			\item SSE4.1
			\item SSE4.2
			\item AVX 256-bit float, double
			\item FMA4 (AMD only)
			\item XOP (AMD only)
			\item FMA3
		\end{itemize}
		\columnbreak
		
		PowerPC/VMX family
		\begin{itemize}
			\item AltiVec 128-bit int8, int16, int32, int64, float
			\item Cell SPU 128-bit int8, int16, int32, int64, float, double
		\end{itemize}
		
		ARM family
		\begin{itemize}
			\item VFP 64-bit float, double
			\item NEON 64-bit and 128-bit float, int8, int16, int32, int64
		\end{itemize}
		
	\vfill
	\end{multicols}
		
\end{frame}

\subsection{Explicit SIMD parallelization}
\begin{frame}
	\frametitle{Why not let the compiler do it?}
	
	\begin{itemize}
		\item Compilers are only so smart
		\item Automatic vectorization can only happen if:
		\begin{itemize}
			\item Memory is well agenced
			\item Code is inherently vectorizable
		\end{itemize}
	\end{itemize}
	
	\begin{itemize}
		\item Compilers don't always have enough static information to know what they can vectorize
		\item Designing for vectorization is a human process
	\end{itemize}
	
	Conclusion:
	\begin{itemize}
		\item Declaring SIMD parallelism explicitly is the best way
              to ensure your code gets vectorized
        \item To be demonstrated by this presentation
    \end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Overview}
	
	\begin{itemize}
		\item Interface and rationale
		\item Interaction with standard tools
		\item SIMD programming idioms
	\end{itemize}
\end{frame}

%\begin{frame}
%  \frametitle{Outline}
%\tableofcontents
  % You might wish to add the option [pausesections]
%\end{frame}

\section{Interface}

\subsection{Hand-written SIMD}
\begin{frame}[fragile]
	\frametitle{Writing it by hand}

	Doing \lstinline{a * b + c} with vectors of 32-bit integers
	\bigskip

	SSE:
	\begin{lstlisting}
	__m128i a, b, c;
	__m128i result = _mm_add_mul_epi32(a, _mm_add_epi32(b, c));
	\end{lstlisting}
	\bigskip
	
	Altivec:
	\begin{lstlisting}
	__vector int a, b, c;
	__vector int result = vec_madd(a, b, c);
	\end{lstlisting}	
	
\end{frame}

\subsection{Pack}

\begin{frame}
	\frametitle{Pack}
	
	\begin{tabular}{ll}
	\lstinline{pack<T, N>} & SIMD register that packs \lstinline{N} elements of type \lstinline{T}\\
	\lstinline{pack<T>} & automatically finds best \lstinline{N} available
	\end{tabular}
	\bigskip	
	
	\begin{quote} Behaves just like \lstinline{T} except operations yield a pack of \lstinline{T} and not a \lstinline{T}. \end{quote}
	\bigskip	
	
	\lstinline{T} must be a fundamental arithmetic type, i.e. (\lstinline{un})\lstinline{signed char}, (\lstinline{unsigned}) \lstinline{short}, (\lstinline{unsigned}) \lstinline{int}, (\lstinline{unsigned}) \lstinline{long}, (\lstinline{unsigned}) \lstinline{long long}, \lstinline{float} or \lstinline{double} -- not \lstinline{bool}.
	\bigskip
	
	\lstinline{N} must be a power of 2.
	
\end{frame}

\subsection{Primitives}

\begin{frame}
	\frametitle{Operators}
	
	\begin{itemize}
		\item All overloadable operators are available
		\item \lstinline{pack<T>} x \lstinline{pack<T>} operations but also \lstinline{pack<T>} x \lstinline{T}
		\item All operators also exist as functions and PFOs
		\item \lstinline{if_else} also available, but evaluation not lazy -- all branches get evaluated
		\item Type coercion and promotion disabled\\
		      \lstinline{uint8_t(255) + uint8_t(1)} yields \lstinline{uint8_t(0)}, not \lstinline{int(256)}
		\item Comparison operators are lexicographic so as to yield booleans,
		      use other functions if you want to do a memberwise comparison.
	\end{itemize}	
	
\end{frame}

\begin{frame}
	\frametitle{Memory access}
	
	Memory must be aligned on \lstinline{sizeof(T)*N} to load/store a \lstinline{pack<T, N>}
	from a \lstinline{T*}\\
	-- except when it is statically known by how many elements it is not aligned.
	
	\begin{itemize}
		\item \lstinline{load< pack<T, N> >(p, i)} loads pack at aligned address \lstinline{p + i*N}
		\item \lstinline{load< pack<T, N>, Offset>(p, i)} loads pack at address \lstinline{p + i*N}, \lstinline{p + Offset} must be aligned
		\item \lstinline{store(p, i, pk)} stores pack \lstinline{pk} at aligned address \lstinline{p + i*N}
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Element access}
	
	\begin{itemize}
		\item like \lstinline{boost::array}, \lstinline{pack} is both a random access fusion sequence and random access range, but read-only
		\item \lstinline{at\_c<i>(p)} or \lstinline{p[i]} can be used to access the i-th element, but is usually slow (\lstinline{at\_c} is faster)
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Proto expressions}
	
	\begin{itemize}
		\item All expressions, even those involving functions,
		      generate template expressions that are evaluated on assignment
		      or in the conversion operator
		\item \lstinline{a * b + c} is mapped to \lstinline{fma(a, b, c)}\\
		      \lstinline{a + b * c} is mapped to \lstinline{fma(b, c, a)}\\
		      \lstinline{!(a < b)} is mapped to \lstinline{is_nle(a, b)}
	\end{itemize}
	
\end{frame}

\begin{frame}
	\frametitle{Extra arithmetic, bitwise and ieee operations, predicates}
	
	\begin{multicols}{3}		
	
		Arithmetic
		\begin{itemize}
			\item saturated arithmetic
			\item float/int conversion
			\item round, floor, ceil, trunc
			\item sqrt, hypot
			\item average
			\item random
			\item min/max
			\item rounded division and remainder
		\end{itemize}

		Bitwise
		\begin{itemize}			
			\item select
			\item andnot, ornot
			\item popcnt
			\item ffs
			\item ror, rol, rshr, rshl
			\item twopower
		\end{itemize}
			
		IEEE
		\begin{itemize}
			\item ilogb, frexp
			\item ldexp
			\item next/prev
			\item ulpdist
		\end{itemize}
			
		Predicates
		\begin{itemize}
			\item comparison with zero
			\item negation of comparison
			\item is\_unord, is\_nan, is\_invalid
			\item is\_odd, is\_even
			\item majority
		\end{itemize}
		
		\vfill
	
	\end{multicols}
	
\end{frame}

\begin{frame}
	\frametitle{Reduction and SWAR operations}
	
	\begin{multicols}{2}	
	
	Reduction
	\begin{itemize}
		\item any, all
		\item nbtrue
		\item minimum/maximum, posmin/posmax
		\item sum
		\item product, dot product
	\end{itemize}
	\columnbreak	
	
	SWAR
	\begin{itemize}
		\item group/split
		\item splatted reduction
		\item cumsum
		\item sort
	\end{itemize}
	
	\vfill
	\end{multicols}
	
\end{frame}

\subsection{Native}
\begin{frame}
	\frametitle{Under the hood}
	
	\begin{tabular}{ll}
	\lstinline{native<T, X>} & SIMD register for type \lstinline{T} on architecture \lstinline{X}
	\end{tabular}
	\bigskip
	
	\begin{itemize}
		\item like \lstinline{pack} but Plain Old Data and all operations and functions return values and not
		      expression templates.
		\item \lstinline{X} characterizes the register type, not the instructions available. Only one tag for all
		      SSE variants.
		\item It is the interface that must be used to extend the library.
	\end{itemize}
	\bigskip
	
	\begin{tabular}{ll}
	\lstinline{native<float, tag::sse_>} & wraps a \lstinline{__m128}\\
	\lstinline{native<uint8_t, tag::sse_>} & wraps a \lstinline{__m128i}\\
	\lstinline{native<double, tag::avx_>} & wraps a \lstinline{__m256d}\\
	\lstinline{native<float, tag::altivec_>} & wraps a \lstinline{__vector float}
	\end{tabular}
	
\end{frame}

\begin{frame}
	\frametitle{Software fallback}
	
	\begin{itemize}
		\item \lstinline{tag::none_<N>} is a software-emulated SIMD architecture with a register size of \lstinline{N} bytes
		\item It is used as fallback when no satisfying SIMD architecture is found
		\item Thanks to this, code can degrade well and remain portable.
	\end{itemize}
	\bigskip	
	
	Default native type when no SIMD is found is\\
	\hspace{10px}\lstinline{native<T, tag::none_<8> >}
	
\end{frame}

\subsection{Set-up}
\begin{frame}
	\frametitle{How to compile the examples?}	
	
	Pre-requisites:
	\begin{itemize}
		\item Python 2.6+
		\item CMake 2.6+
		\item Git 1.6+
		\item Boost 1.46+ or SVN trunk
		\item $NT^2$ git master
		\item Preferably a Linux/x86/GCC setup with GCC 4.6+
	\end{itemize}
	\bigskip
	
	\texttt{BOOST\_ROOT} must point to Boost\\
	\texttt{NT2\_SOURCE\_ROOT} must point to $NT^2$
	\bigskip
	
	Examples are at:\\
	\texttt{git://github.com/MetaScale/boost-con-2011.git}
	
\end{frame}

\subsection{RGB to grayscale}
\begin{frame}[fragile]
	\frametitle{RGB to grayscale}
	
	Data:
	\begin{lstlisting}
	float const *red, *green, *blue;
	float* result;
	\end{lstlisting}
	
	Scalar version:
	\begin{lstlisting}
	for(std::size_t i = 0; i != height*width; ++i)
	    result[i] = 0.3f * red[i] + 0.59f * green[i] + 0.11f * blue[i]; 
	\end{lstlisting}
	
\end{frame}

\begin{frame}[fragile]
	\frametitle{SIMD version}
	
	\begin{lstlisting}
	static const std::size_t N = meta::cardinal_of< pack<float> >::value;
	for(std::size_t i = 0; i != height*width/N; ++i)
	{
	    pack<float> r = load< pack<float> >(red, i);
	    pack<float> g = load< pack<float> >(green, i);
	    pack<float> b = load< pack<float> >(blue, i);
		
	    pack<float> res = 0.3f * r + 0.59f * g + 0.11f * b;
	    store(res, result, i);
	}
	\end{lstlisting}
	
	SIMD version assumes:
	\begin{itemize}
		\item red, green, blue and result are aligned on a \lstinline{N} boundary
		\item width is a multiple of \lstinline{N} -- padding your n-dimensional structures is strongly recommended
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Easy enough, but what if...}

	\begin{itemize}
		\item ... i've got interleaved RGB or RGBA?
		\item ... i've got 8-bit integers and not floats?
	\end{itemize}
	
	Can be complicated, we'll see that later.
	
\end{frame}

\include{simdstl}

\include{simdspec}

\begin{frame}
	\frametitle{Back to RGB to Grayscale}
	
	\begin{itemize}
		\item 8-bit RGB, separate channels
		\item float interleaved RGBA
	\end{itemize}
	
\end{frame}

\begin{frame}[fragile]
	\frametitle{8-bit RGB}
	
	\begin{lstlisting}
	static const std::size_t N = meta::cardinal_of< pack<uint8_t> >::value;
	for(std::size_t i = 0; i != height*width/N; ++i)
	{
	    pack<uint8_t> r = load< pack<uint8_t> >(red, i);
	    pack<uint8_t> g = load< pack<uint8_t> >(green, i);
	    pack<uint8_t> b = load< pack<uint8_t> >(blue, i);
		
	    pack<uint8_t> res = uint8_t(77) * r / uint8_t(255) + uint8_t(150) * g / uint8_t(255) + uint8_t(28) * b / uint8_t(255);
	    store(res, result, i);
	}
	\end{lstlisting}
\end{frame}

\begin{frame}
	\frametitle{Dealing with overflow}
	
	Two solutions:
	\begin{itemize}
		\item Promote to int16 -- or even to int32 and convert to float if you want to reuse the previous coefficients
		\item Equilibrate the coefficients and use saturated arithmetic
	\end{itemize} 	

\end{frame}

\begin{frame}[fragile]
	\frametitle{Promote the pack}
	
	\begin{lstlisting}
	uint16_t r_coeff = 77;
	uint16_t g_coeff = 150;
	uint16_t b_coeff = 28;
	uint16_t div_coeff = 255;
	\end{lstlisting}
	
	\begin{lstlisting}	
	pack<uint16_t> r1, r2, g1, g2, b1, b2;
	tie(r1, r2) = split(r);
	tie(g1, g2) = split(g);
	tie(b1, b2) = split(b);
	
	pack<uint16_t> res1 = r_coeff * r1 / div_coeff + g_coeff * g1 / div_coeff + b_coeff * b1 / div_coeff;
	pack<uint16_t> res2 = r_coeff * r2 / div_coeff + g_coeff * g2 / div_coeff + b_coeff * b2 / div_coeff;
	
	pack<uint8_t> res = group(res1, res2);
	\end{lstlisting}
	
\end{frame}

\begin{frame}[fragile]
	\frametitle{Saturated version -- not really a good idea}
	
	\begin{lstlisting}
	uint8_t r_coeff = 26;
	uint8_t g_coeff = 50;
	uint8_t b_coeff = 9;
	uint8_t div_coeff = 85;
	\end{lstlisting}
	
	\begin{lstlisting}
	pack<uint8_t> res = adds(adds(muls(r_coeff, r / div_coeff), muls(g_coeff, g / div_coeff)), muls(b_coeff, b / div_coeff));
	\end{lstlisting}
	
\end{frame}

\begin{frame}[fragile]
	\frametitle{Interleaved RGBA (don't do it)}
	
	Data:
	\begin{lstlisting}
	float const* image;
	float* result;
	\end{lstlisting}
	\bigskip	
	
	Scalar version:
	\begin{lstlisting}
	for(std::size_t i = 0; i != height*width; i += 4)
	    result[i] = 0.3f * image[i] + 0.59 * image[i+1] + 0.11 * image[i+2];
	\end{lstlisting}
	
\end{frame}

\begin{frame}[fragile]
	\frametitle{SIMD version}
	
	\begin{lstlisting}
	static const std::size_t N = meta::cardinal_of< pack<float> >::value;
	for(std::size_t i = 0; i != height*width/N; i += 4)
	{
	    pack<float> rgba1 = load< pack<float> >(image, i);
	    pack<float> rgba2 = load< pack<float> >(image, i+1);
	    pack<float> rgba3 = load< pack<float> >(image, i+2);
	    pack<float> rgba4 = load< pack<float> >(image, i+3);
	    
	    _MM_TRANSPOSE4_PS(rgba1, rgba2, rgba3, rgba4);		
	    
	    pack<float> res1 = 0.3f * rgba1 + 0.59 * rgba2 + 0.11 * rgba3;
	    store(res, result, i/4); 
	}
	\end{lstlisting}
	\bigskip
	
	\lstinline{transpose} is \textbf{not} mapped in Boost.SIMD because it's just \textit{wrong}
\end{frame}

\begin{frame}
	\frametitle{Timings}
	
	
\end{frame}

\section{Conclusion}
\frame
{
  \frametitle{Overview of Boost.SIMD}
}

\frame
{
  \frametitle{Upcoming works}
}

\frame
{
\begin{center}\Huge Thanks for your attention\end{center}
}

\end{document}
